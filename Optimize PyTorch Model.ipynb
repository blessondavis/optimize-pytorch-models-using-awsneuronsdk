{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63bbb356",
   "metadata": {},
   "source": [
    "# Optimizing PyTorch Model for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb5d00b",
   "metadata": {},
   "source": [
    "### Install and Import the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "058d3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --no-cache-dir torch-neuron neuron-cc[tensorflow] torchvision torch --extra-index-url=https://pip.repos.neuron.amazonaws.com\n",
    "!pip install --upgrade --no-cache-dir 'transformers==4.6.0'\n",
    "!pip install accelerate==0.20.3'\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a82e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_neuron\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855b32c3",
   "metadata": {},
   "source": [
    "### Compile and Save the Model into TorchScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22be6666",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1f717969f341a18d186b0f478ea5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad5e27848224a939e6d62c5239c0463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2dd29d9f8b74d04aa917468992c0718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ec544600a94bf7be91e86eb796c092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50d26f5dced46e78aadd058f3369b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gchhablani/bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"gchhablani/bert-base-cased-finetuned-mrpc\", return_dict=False)\n",
    "\n",
    "# Setup some example inputs\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "max_length=128\n",
    "paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Run the original PyTorch model on compilation exaple\n",
    "paraphrase_classification_logits = model(**paraphrase)[0]\n",
    "\n",
    "# Convert example inputs to a format that is compatible with TorchScript tracing\n",
    "example_inputs_paraphrase = paraphrase['input_ids'], paraphrase['attention_mask'], paraphrase['token_type_ids']\n",
    "example_inputs_not_paraphrase = not_paraphrase['input_ids'], not_paraphrase['attention_mask'], not_paraphrase['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4284cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run torch.neuron.trace to generate a TorchScript that is optimized by AWS Neuron\n",
    "model_neuron = torch.neuron.trace(model, example_inputs_paraphrase, verbose=1, compiler_workdir='./compilation_artifacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14932e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TorchScript for later use\n",
    "model_neuron.save('neuron_compiled_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e976c54e",
   "metadata": {},
   "source": [
    "### Upload the Compiled Model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6e65127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "neuron_compiled_model.pt\r\n"
     ]
    }
   ],
   "source": [
    "# Create a model.tar.gz file to be used by SageMaker endpoint\n",
    "!tar -czvf model.tar.gz neuron_compiled_model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "413a07ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "from sagemaker.utils import name_from_base\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9193ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload model to S3\n",
    "role = sagemaker.get_execution_role()\n",
    "sess=sagemaker.Session()\n",
    "region=sess.boto_region_name\n",
    "bucket=sess.default_bucket()\n",
    "sm_client=boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a94bd0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded model to S3:\n",
      "s3://sagemaker-ap-south-1-128015641074/inf1_compiled_model/model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_key = '{}/model/model.tar.gz'.format('inf1_compiled_model')\n",
    "model_path = 's3://{}/{}'.format(bucket, model_key)\n",
    "boto3.resource('s3').Bucket(bucket).upload_file('model.tar.gz', model_key)\n",
    "print(\"Uploaded model to S3:\")\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80657ecd",
   "metadata": {},
   "source": [
    "### Install Custom Libraries in the PyTorch Container and Push to ECR\n",
    "We use Sagemaker Prebuilt Container here and install the transformers libraries. You can follow the similar approach to install any custom libraries required by your inference code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89ee0cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference-neuron:1.7.1-neuron-py36-ubuntu18.04\r\n",
      "\r\n",
      "# Install packages\r\n",
      "RUN pip install \"transformers==4.7.0\""
     ]
    }
   ],
   "source": [
    "!cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7854d992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  2.048kB\n",
      "Step 1/2 : FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference-neuron:1.7.1-neuron-py36-ubuntu18.04\n",
      "1.7.1-neuron-py36-ubuntu18.04: Pulling from pytorch-inference-neuron\n",
      "4bbfd2c87b75: Pulling fs layer\n",
      "d2e110be24e1: Pulling fs layer\n",
      "889a7173dcfe: Pulling fs layer\n",
      "3575d41c0835: Pulling fs layer\n",
      "bc6f168604e2: Pulling fs layer\n",
      "eba9221f33a4: Pulling fs layer\n",
      "dc17b7738357: Pulling fs layer\n",
      "edfa9cb4c511: Pulling fs layer\n",
      "a3ae7b373871: Pulling fs layer\n",
      "72c61eda0de1: Pulling fs layer\n",
      "81938fffccdc: Pulling fs layer\n",
      "d15411504276: Pulling fs layer\n",
      "2e5382e0d0b4: Pulling fs layer\n",
      "e636fe876326: Pulling fs layer\n",
      "28a1c6f31ec5: Pulling fs layer\n",
      "bc6f168604e2: Waiting\n",
      "eba9221f33a4: Waiting\n",
      "af520a0bb6c1: Pulling fs layer\n",
      "177f01cd03de: Pulling fs layer\n",
      "3575d41c0835: Waiting\n",
      "edfa9cb4c511: Waiting\n",
      "c4f0810c7324: Pulling fs layer\n",
      "120debb0c6f1: Pulling fs layer\n",
      "81938fffccdc: Waiting\n",
      "a3ae7b373871: Waiting\n",
      "2e5382e0d0b4: Waiting\n",
      "607d59193173: Pulling fs layer\n",
      "e636fe876326: Waiting\n",
      "f512442d20c1: Pulling fs layer\n",
      "dc17b7738357: Waiting\n",
      "177f01cd03de: Waiting\n",
      "af520a0bb6c1: Waiting\n",
      "607d59193173: Waiting\n",
      "c4f0810c7324: Waiting\n",
      "120debb0c6f1: Waiting\n",
      "b86ad2651d33: Pulling fs layer\n",
      "013d5c197e30: Pulling fs layer\n",
      "b86ad2651d33: Waiting\n",
      "013d5c197e30: Waiting\n",
      "d2e110be24e1: Verifying Checksum\n",
      "d2e110be24e1: Download complete\n",
      "889a7173dcfe: Verifying Checksum\n",
      "889a7173dcfe: Download complete\n",
      "bc6f168604e2: Download complete\n",
      "4bbfd2c87b75: Verifying Checksum\n",
      "4bbfd2c87b75: Download complete\n",
      "eba9221f33a4: Download complete\n",
      "4bbfd2c87b75: Pull complete\n",
      "d2e110be24e1: Pull complete\n",
      "889a7173dcfe: Pull complete\n",
      "edfa9cb4c511: Download complete\n",
      "dc17b7738357: Verifying Checksum\n",
      "dc17b7738357: Download complete\n",
      "a3ae7b373871: Verifying Checksum\n",
      "a3ae7b373871: Download complete\n",
      "3575d41c0835: Verifying Checksum\n",
      "3575d41c0835: Download complete\n",
      "3575d41c0835: Pull complete\n",
      "72c61eda0de1: Verifying Checksum\n",
      "72c61eda0de1: Download complete\n",
      "bc6f168604e2: Pull complete\n",
      "eba9221f33a4: Pull complete\n",
      "dc17b7738357: Pull complete\n",
      "edfa9cb4c511: Pull complete\n",
      "2e5382e0d0b4: Verifying Checksum\n",
      "2e5382e0d0b4: Download complete\n",
      "81938fffccdc: Verifying Checksum\n",
      "81938fffccdc: Download complete\n",
      "e636fe876326: Verifying Checksum\n",
      "e636fe876326: Download complete\n",
      "28a1c6f31ec5: Verifying Checksum\n",
      "28a1c6f31ec5: Download complete\n",
      "af520a0bb6c1: Verifying Checksum\n",
      "af520a0bb6c1: Download complete\n",
      "177f01cd03de: Download complete\n",
      "c4f0810c7324: Verifying Checksum\n",
      "c4f0810c7324: Download complete\n",
      "120debb0c6f1: Verifying Checksum\n",
      "120debb0c6f1: Download complete\n",
      "a3ae7b373871: Pull complete\n",
      "f512442d20c1: Verifying Checksum\n",
      "f512442d20c1: Download complete\n",
      "607d59193173: Verifying Checksum\n",
      "607d59193173: Download complete\n",
      "b86ad2651d33: Verifying Checksum\n",
      "b86ad2651d33: Download complete\n",
      "013d5c197e30: Verifying Checksum\n",
      "013d5c197e30: Download complete\n",
      "72c61eda0de1: Pull complete\n",
      "81938fffccdc: Pull complete\n",
      "d15411504276: Verifying Checksum\n",
      "d15411504276: Download complete\n",
      "d15411504276: Pull complete\n",
      "2e5382e0d0b4: Pull complete\n",
      "e636fe876326: Pull complete\n",
      "28a1c6f31ec5: Pull complete\n",
      "af520a0bb6c1: Pull complete\n",
      "177f01cd03de: Pull complete\n",
      "c4f0810c7324: Pull complete\n",
      "120debb0c6f1: Pull complete\n",
      "607d59193173: Pull complete\n",
      "f512442d20c1: Pull complete\n",
      "b86ad2651d33: Pull complete\n",
      "013d5c197e30: Pull complete\n",
      "Digest: sha256:82aa69f16a74d4e052116d32c751d1a12539e06730ab63915b6da26c5d982fd6\n",
      "Status: Downloaded newer image for 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference-neuron:1.7.1-neuron-py36-ubuntu18.04\n",
      " ---> 388bfe7d2429\n",
      "Step 2/2 : RUN pip install \"transformers==4.7.0\"\n",
      " ---> Running in 5656332b5c04\n",
      "Collecting transformers==4.7.0\n",
      "  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.7.0) (20.4)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.4.1-py3-none-any.whl (9.9 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.7.0) (2.22.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.7.0) (4.4.0)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.7.0) (0.8)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from transformers==4.7.0) (5.4.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.8.8-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (759 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.7.0) (1.17.4)\n",
      "Collecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.7.0) (4.59.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.7.0) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.7.0) (3.10.0.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.7.0) (1.16.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.7.0) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.7.0) (2021.5.30)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.7.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.7.0) (1.25.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.7.0) (2.8)\n",
      "Collecting click\n",
      "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.7.0) (1.0.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895254 sha256=6b6d4a353174e83debd12b418c2a8ee3bdc14914f2b30cff76c290f3aeed6d4f\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/64/31/e9900a234b23fb3e9dc565d6114a9d6ff84a72dbdd356502b4\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: regex, filelock, click, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed click-8.0.4 filelock-3.4.1 huggingface-hub-0.0.8 regex-2023.8.8 sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.7.0\n",
      "\u001b[91mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 5656332b5c04\n",
      " ---> 1211b4f96a2c\n",
      "[Warning] One or more build-args [REGION] were not consumed\n",
      "Successfully built 1211b4f96a2c\n",
      "Successfully tagged neuron-py36-inference:latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "The push refers to repository [128015641074.dkr.ecr.ap-south-1.amazonaws.com/neuron-py36-inference]\n",
      "19994d4b34ab: Preparing\n",
      "5d32c72ad027: Preparing\n",
      "f4d9427d752b: Preparing\n",
      "62b8cb6215cb: Preparing\n",
      "877b36f2f41c: Preparing\n",
      "4beb4e23ce0b: Preparing\n",
      "629f76e0ebfa: Preparing\n",
      "55aafc4b4134: Preparing\n",
      "5b505b65f8e8: Preparing\n",
      "4383d9750962: Preparing\n",
      "120ef6a75dae: Preparing\n",
      "d1a63e051735: Preparing\n",
      "1353ff378dc3: Preparing\n",
      "3d7573db3c3f: Preparing\n",
      "2858c813c4e4: Preparing\n",
      "4beb4e23ce0b: Waiting\n",
      "e8b427e8fb51: Preparing\n",
      "6b9a1856b2e9: Preparing\n",
      "5b505b65f8e8: Waiting\n",
      "629f76e0ebfa: Waiting\n",
      "79ec63999885: Preparing\n",
      "5dcdeb94f6a5: Preparing\n",
      "b8b74f1e44f0: Preparing\n",
      "4383d9750962: Waiting\n",
      "bf9a431aeda6: Preparing\n",
      "5f08512fd434: Preparing\n",
      "120ef6a75dae: Waiting\n",
      "55aafc4b4134: Waiting\n",
      "c7bb31fc0e08: Preparing\n",
      "3d7573db3c3f: Waiting\n",
      "d1a63e051735: Waiting\n",
      "50858308da3d: Preparing\n",
      "2858c813c4e4: Waiting\n",
      "5dcdeb94f6a5: Waiting\n",
      "b8b74f1e44f0: Waiting\n",
      "6b9a1856b2e9: Waiting\n",
      "bf9a431aeda6: Waiting\n",
      "5f08512fd434: Waiting\n",
      "c7bb31fc0e08: Waiting\n",
      "1353ff378dc3: Waiting\n",
      "5d32c72ad027: Pushed\n",
      "f4d9427d752b: Pushed\n",
      "62b8cb6215cb: Pushed\n",
      "877b36f2f41c: Pushed\n",
      "55aafc4b4134: Pushed\n",
      "629f76e0ebfa: Pushed\n",
      "4beb4e23ce0b: Pushed\n",
      "5b505b65f8e8: Pushed\n",
      "4383d9750962: Pushed\n",
      "120ef6a75dae: Pushed\n",
      "d1a63e051735: Pushed\n",
      "19994d4b34ab: Pushed\n",
      "6b9a1856b2e9: Pushed\n",
      "79ec63999885: Pushed\n",
      "5dcdeb94f6a5: Pushed\n",
      "b8b74f1e44f0: Pushed\n",
      "3d7573db3c3f: Pushed\n",
      "5f08512fd434: Pushed\n",
      "c7bb31fc0e08: Pushed\n",
      "50858308da3d: Pushed\n",
      "e8b427e8fb51: Pushed\n",
      "2858c813c4e4: Pushed\n",
      "bf9a431aeda6: Pushed\n",
      "1353ff378dc3: Pushed\n",
      "latest: digest: sha256:4b338eeade24850ca3a69699b8f3d5a8fff871d09cd872bb3789760284e87567 size: 5348\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=neuron-py36-inference\n",
    "cd container\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-ap-south-1}\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR in order to pull down the SageMaker PyTorch image\n",
    "aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR with the full name.\n",
    "docker build  -t ${algorithm_name} . --build-arg REGION=${region}\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region} | docker login --username AWS --password-stdin ${account}.dkr.ecr.${region}.amazonaws.com\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4109a9a",
   "metadata": {},
   "source": [
    "### Deploying the Model using the Extended ECR Container Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5d22bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d18342b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128015641074.dkr.ecr.ap-south-1.amazonaws.com/neuron-py36-inference:latest\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the container image that we pushed to ECR in the above step\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"inf1_compiled_model/model\"\n",
    "\n",
    "# Get container name in ECR\n",
    "client=boto3.client('sts')\n",
    "account=client.get_caller_identity()['Account']\n",
    "\n",
    "my_session=boto3.session.Session()\n",
    "region=my_session.region_name\n",
    "\n",
    "algorithm_name=\"neuron-py36-inference\"\n",
    "ecr_image='{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, algorithm_name)\n",
    "print(ecr_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3aa97728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-south-1-128015641074/inf1_compiled_model/model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "key = os.path.join(prefix, \"model.tar.gz\")\n",
    "pretrained_model_data = \"s3://{}/{}\".format(bucket, key)\n",
    "print(pretrained_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03fc6c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch Estimator using the inference code and the ECR image\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=pretrained_model_data,\n",
    "    role=role,\n",
    "    source_dir=\"code\",\n",
    "    framework_version=\"1.7.1\",\n",
    "    entry_point=\"inference.py\",\n",
    "    image_uri=ecr_image\n",
    ")\n",
    "\n",
    "# Inform SageMaker that the model has been compiled using neuron-cc. \n",
    "pytorch_model._is_compiled_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "697c6a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "predictor = pytorch_model.deploy(initial_instance_count=1, instance_type=\"ml.inf1.2xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f1eaff",
   "metadata": {},
   "source": [
    "#### Note\n",
    "*In the input_fn, we specified JSON encoding for incoming requests, so we must employ a JSON serializer to encode the data. Additionally, since we set the return content type as a JSON string, a JSON deserializer is required to parse the response.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c187d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c779611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT says that \"Never allow the same bug to bite you twice.\" and \"The best part of Amazon SageMaker is that it makes machine learning easy.\" are not paraphrase\n",
      "CPU times: user 11.8 ms, sys: 6.6 ms, total: 18.4 ms\n",
      "Wall time: 146 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = predictor.predict(\n",
    "    [\n",
    "        \"Never allow the same bug to bite you twice.\",\n",
    "        \"The best part of Amazon SageMaker is that it makes machine learning easy.\",\n",
    "    ]\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "14ecd116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT says that \"The company HuggingFace is based in New York City\" and \"HuggingFace's headquarters are situated in Manhattan\" are paraphrase\n",
      "CPU times: user 18.3 ms, sys: 96 µs, total: 18.4 ms\n",
      "Wall time: 64.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = predictor.predict(\n",
    "    [\n",
    "        \"The company HuggingFace is based in New York City\",\n",
    "        \"HuggingFace's headquarters are situated in Manhattan\",\n",
    "    ]\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca01a03",
   "metadata": {},
   "source": [
    "### (Optional) Benchmarking Endpoints Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e15ba75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28f3996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_latency(model,*inputs):\n",
    "    \"\"\"\n",
    "    infetence_time is a simple method to return the latency of a model inference.\n",
    "\n",
    "        Parameters:\n",
    "            model: torch model onbject loaded using torch.jit.load\n",
    "            inputs: model() args\n",
    "\n",
    "        Returns:\n",
    "            latency in seconds\n",
    "    \"\"\"\n",
    "    error = False\n",
    "    start = time.time()\n",
    "    try:\n",
    "        results = model(*inputs)\n",
    "    except:\n",
    "        error = True\n",
    "        results = []\n",
    "    return {'latency':time.time() - start, 'error': error, 'result': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9cbdbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The king retards a sloth', 'A sloth configures superman']\n"
     ]
    }
   ],
   "source": [
    "def random_sentence():\n",
    "\n",
    "    s_nouns = [\"A dude\", \"My mom\", \"The king\", \"Some guy\", \"A cat with rabies\", \"A sloth\", \"Your homie\", \"This cool guy my gardener met yesterday\", \"Superman\"]\n",
    "    p_nouns = [\"These dudes\", \"Both of my moms\", \"All the kings of the world\", \"Some guys\", \"All of a cattery's cats\", \"The multitude of sloths living under your bed\", \"Your homies\", \"Like, these, like, all these people\", \"Supermen\"]\n",
    "    s_verbs = [\"eats\", \"kicks\", \"gives\", \"treats\", \"meets with\", \"creates\", \"hacks\", \"configures\", \"spies on\", \"retards\", \"meows on\", \"flees from\", \"tries to automate\", \"explodes\"]\n",
    "    p_verbs = [\"eat\", \"kick\", \"give\", \"treat\", \"meet with\", \"create\", \"hack\", \"configure\", \"spy on\", \"retard\", \"meow on\", \"flee from\", \"try to automate\", \"explode\"]\n",
    "    infinitives = [\"to make a pie.\", \"for no apparent reason.\", \"because the sky is green.\", \"for a disease.\", \"to be able to make toast explode.\", \"to know more about archeology.\"]\n",
    "\n",
    "    return (random.choice(s_nouns) + ' ' + random.choice(s_verbs) + ' ' + random.choice(s_nouns).lower() or random.choice(p_nouns).lower() + ' ' + random.choice(infinitives))\n",
    "\n",
    "print([random_sentence(), random_sentence()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea5bbdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:09<00:00, 107.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.36 s, sys: 102 ms, total: 1.46 s\n",
      "Wall time: 9.33 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Defining Auxiliary variables\n",
    "number_of_clients = 2\n",
    "number_of_runs = 1000\n",
    "t = tqdm(range(number_of_runs),position=0, leave=True)\n",
    "\n",
    "# Starting parallel clients\n",
    "cw_start = datetime.datetime.utcnow()\n",
    "\n",
    "results = Parallel(n_jobs=number_of_clients,prefer=\"threads\")(delayed(inference_latency)(predictor.predict,[random_sentence(), random_sentence()]) for mod in t)\n",
    "avg_throughput = t.total/t.format_dict['elapsed']\n",
    "\n",
    "cw_end = datetime.datetime.utcnow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c52a315b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Throughput: :107.2\n",
      "\n",
      "50th Percentile Latency:18.4 ms\n",
      "90th Percentile Latency:19.3 ms\n",
      "95th Percentile Latency:21.9 ms\n",
      "\n",
      "Errors percentage: 0.0 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Computing metrics and print\n",
    "latencies = [res['latency'] for res in results]\n",
    "errors = [res['error'] for res in results]\n",
    "error_p = sum(errors)/len(errors) *100\n",
    "p50 = np.quantile(latencies[-1000:],0.50) * 1000\n",
    "p90 = np.quantile(latencies[-1000:],0.95) * 1000\n",
    "p95 = np.quantile(latencies[-1000:],0.99) * 1000\n",
    "\n",
    "print(f'Avg Throughput: :{avg_throughput:.1f}\\n')\n",
    "print(f'50th Percentile Latency:{p50:.1f} ms')\n",
    "print(f'90th Percentile Latency:{p90:.1f} ms')\n",
    "print(f'95th Percentile Latency:{p95:.1f} ms\\n')\n",
    "print(f'Errors percentage: {error_p:.1f} %\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2593b45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Cloudwatch:\n",
      "Time elapsed: 309.535067 seconds\n",
      "Using period of 360 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Querying CloudWatch\n",
    "print('Getting Cloudwatch:')\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "statistics=['SampleCount', 'Average', 'Minimum', 'Maximum']\n",
    "extended=['p50', 'p90', 'p95', 'p100']\n",
    "\n",
    "# Give 5 minute buffer to end\n",
    "cw_end += datetime.timedelta(minutes=5)\n",
    "\n",
    "# Period must be 1, 5, 10, 30, or multiple of 60\n",
    "# Calculate closest multiple of 60 to the total elapsed time\n",
    "factor = math.ceil((cw_end - cw_start).total_seconds() / 60)\n",
    "period = factor * 60\n",
    "print('Time elapsed: {} seconds'.format((cw_end - cw_start).total_seconds()))\n",
    "print('Using period of {} seconds\\n'.format(period))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f76c3ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 30 seconds ...\n",
      "1000.0 latency datapoints ready\n",
      "50th Percentile Latency:14.2 ms\n",
      "90th Percentile Latency:15.0 ms\n",
      "95th Percentile Latency:15.1 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cloudwatch_ready = False\n",
    "# Keep polling CloudWatch metrics until datapoints are available\n",
    "while not cloudwatch_ready:\n",
    "  time.sleep(30)\n",
    "  print('Waiting 30 seconds ...')\n",
    "  # Must use default units of microseconds\n",
    "  model_latency_metrics = cloudwatch.get_metric_statistics(MetricName='ModelLatency',\n",
    "                                             Dimensions=[{'Name': 'EndpointName',\n",
    "                                                          'Value': predictor.endpoint_name},\n",
    "                                                         {'Name': 'VariantName',\n",
    "                                                          'Value': \"AllTraffic\"}],\n",
    "                                             Namespace=\"AWS/SageMaker\",\n",
    "                                             StartTime=cw_start,\n",
    "                                             EndTime=cw_end,\n",
    "                                             Period=period,\n",
    "                                             Statistics=statistics,\n",
    "                                             ExtendedStatistics=extended\n",
    "                                             )\n",
    "  # Should be 1000\n",
    "  if len(model_latency_metrics['Datapoints']) > 0:\n",
    "    print('{} latency datapoints ready'.format(model_latency_metrics['Datapoints'][0]['SampleCount']))\n",
    "    side_avg = model_latency_metrics['Datapoints'][0]['Average'] / number_of_runs\n",
    "    side_p50 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p50'] / number_of_runs\n",
    "    side_p90 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p90'] / number_of_runs\n",
    "    side_p95 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p95'] / number_of_runs\n",
    "    side_p100 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p100'] / number_of_runs\n",
    "\n",
    "    print(f'50th Percentile Latency:{side_p50:.1f} ms')\n",
    "    print(f'90th Percentile Latency:{side_p90:.1f} ms')\n",
    "    print(f'95th Percentile Latency:{side_p95:.1f} ms\\n')\n",
    "\n",
    "    cloudwatch_ready = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4321be8a",
   "metadata": {},
   "source": [
    "#### Delete the endpoints and resources to avoid charges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189277fa",
   "metadata": {},
   "source": [
    "For additional readings, you may refer to the Neuron SDK Documentation page: https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac7f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
